[{"id":0,"href":"/docs/tsnperf/principle/","title":"TSNPerf 设计原理","section":"ZIGGO TSNPerf","content":"\rTSNPerf 设计原理\r#\r现有方案的局限\r#\rTSNPerf是一款专为时间敏感网络（Time-Sensitive Networking, TSN）设计的性能评测工具。它提供了全面的TSN网络性能评测功能，能够帮助网络工程师和研究人员深入了解并验证TSN网络设备的性能特征。为此，TSNPerf需要能够精准地向网络中发送关键数据包，以测试设备的时间同步性能和流量整形能力。然而，Linux操作系统提供的send()函数只能达到微秒级的发包精度，难以满足TSNPerf对高精度发包的要求。\n为了直观地展示send()函数的发包精度限制，我们进行了一项测试。在测试中，我们使用send()函数以1毫秒为设定间隔，连续发送了10000个数据包。然后，我们记录了每两个相邻数据包之间的实际时间间隔，并计算了它们与预期的1毫秒间隔之间的误差。结果显示，即使没有任何背景进程/流量的干扰，相邻数据包的间隔任然达到了最高14微秒左右。接下来我们分析该现象的原因。\n如下图所示，在 $t_0$ 时刻，程序调用 send() 函数向网络发送数据包，但实际上，数据包在 $t_1$ 时刻才会被发送到网络中去。这是因为，操作系统的网络协议栈需要花费一定的时间处理该数据包。我们将 $t_1-t_0$ 称之为网络栈的处理时延，这个时延通常在微秒级别。\n解决方案\r#\rIntel 的 I210 / I225 / I226 网卡，提供了 LaunchTime 功能，可以精准地控制数据包离开网口的时间（理论精度在 32 纳秒级别）。\n利用 LaunchTime，可以提高数据包的发送精度。如上图所示，对于一个需要在 $t_0$ 时刻发出的数据包，我们预留一段时间给网络栈的处理流程，提前在 $t_2$ 时刻就调用 send() 函数，保证数据包在 $t_0$ 前就在网卡的出队列口等待，LaunchTime 会在$t_0$时刻打开出队列的门，将数据包发出。\n性能验证\r#\r类似地，我们开启 LaunchTime 功能，以1毫秒为设定间隔连续发送10000个数据包，记录每两个相邻数据包之间的实际时间间隔与1毫秒之间的误差。 结果如下图所示，所有数据包的发送时间误差不超过100纳秒，达到了TSNPerf对发包精度的要求\n"},{"id":1,"href":"/docs/switch/basic_knowledge/","title":"基础知识列表","section":"ZIGGO Switch","content":"\r基础知识列表\r#\r我们再次重申，使用或者开发ZIGGO-CaaS-Switch或者ZIGGO-Device是有较高门槛的，您必须掌握一定的软硬件知识。如果您之间没有任何的硬件开发经验，您可能需要慎重考虑自己是否适合这个项目，因为学习这些知识是非常花时间的。\n学习使用ZIGGO项目比学习FPGA、MCU、ARM等传统开发工具要求更高，想如臂指使一般地使用ZIGGO设备也不是一蹴而就的事情。\n如果您希望使用ZIGGO项目，您至少应该有以下知识：\n硬件知识\r#\r计算机组成原理 C语言 数字电路基础 Verilog、VHDL语言 良好的英语阅读基础 软件知识\r#\r计算机组成原理 计算机网络 计算机操作系统 C、C++语言 tcl脚本 良好的英语阅读基础 "},{"id":2,"href":"/docs/switch/require/","title":"准备清单","section":"ZIGGO Switch","content":"\r准备清单\r#\r软件环境清单\r#\rVivada 开发环境\r#\rVivado 是开发板的硬件开发程序。\nVivado 软件的 Xilinx 官方下载地址： http://china.xilinx.com/support/download.html\n本开发团队使用的是Vivado 2020.1版本，我们建议ZIGGO项目使用者也使用相同版本以免一些不必要的麻烦。\n具体的安装流程可以参照 ZYNQ应用教程 第三章Vivado开发环境 进行安装。\nMobaXterm\r#\rMobaXterm是串口连接工具，我们在hardware-build中“Launch the Board and log in”这一章节进行了使用介绍。\nMobaXterm的官方下载地址：MobaXterm Xserver with SSH, telnet, RDP, VNC and X11 - Download (mobatek.net)\n硬件准备清单\r#\rAlinx7021开发板\r#\rSD启动卡\r#\rSD启动卡中存储着操作系统，建议容量 $\\geq$ 32G。\n必要的线材\r#\r包括网线、数据线等。\n普通交换机\r#\r如果使用的开发板数量较多，可以考虑将所有开发板连接到普通交换机再通过普通交换机连接至PC（通过网线）进行管理和使用。\n"},{"id":3,"href":"/docs/switch/system-design/","title":"System Design","section":"ZIGGO Switch","content":"\rSystem Design\r#\rThe CaaS/TSN Switch is developed based on the ZYNQ platform. The diagram below shows the composition of the ZYNQ chip. ZYNQ mainly consists of a Processing System (PS) and Programmable Logic (PL). The PS and PL mainly communicate with each other through the high-performance Advanced eXtensible Interface (AXI), which is more efficient than using FPGA directly as a peripheral. The PS contains an ARM-based processor suitable for running applications, drivers, and operating systems, while the PL contains FPGA suitable for running low-level hardware logic with high real-time performance requirements.\nFor the Switch, the PL part mainly implements:\nthe real-time clock module and timestamp cache module in time synchronization; the basic forwarding function and traffic control function of the switch. The PS part mainly implements:\nthe state machine logic related to time synchronization; the configuration program of the switch. Time Synchronization\r#\rPlease refer to the Wikipedia for basic knowledge about clock synchronization .\nThe TSN switch complies with IEEE 802.1AS standards. It synchronize neighbor clocks in a decentralized manner and achieves clock accuracy in the sub-microsecond range, making it suitable for measurement and control systems. For each pair of connected devices, their time synchronization state machine will measure link delay and update their local RTC (real-time clock) according to Master clock.\nThe overall design of the time synchronization module is shown in the diagram below. The PS mainly consists of time synchronization state machine modules, which mainly run the state machine logic defined in the 802.1AS standard. The PL mainly consists of real-time clock modules and timestamp cache modules, which are mainly responsible for running the real-time clock and recording the time when data frames enter and exit the switch port.\nAfter the data frame enters the PL from the input port of the switch, the timestamp cache module will record and cache the timestamp when the data frame enters the hardware. The switch\u0026rsquo;s exchange module will determine whether the data frame is related to time synchronization. Time synchronization data frames will be forwarded from the PL to the PS for processing through the Direct Memory Access (DMA) channel. In the PS, the time synchronization state machine module needs to obtain the real-time clock information of the underlying PL and the hardware timestamps corresponding to different data frames through the AXI4-Lite interface. When the switch needs to send time synchronization-related data frames, the PS is responsible for encapsulating the sent data frames and then forwarding them to the PL for processing through the DMA channel. Since time synchronization also needs to record the sending time of messages such as Sync and Pdelay_Req, the timestamp cache module will still cache the sending timestamp before the data frame is sent from the output port, so that the PS can use it later.\nSwitch Fabric \u0026amp; Gate Control\r#\rThe overall design of the switch fabric and gate control module is shown in the diagram below. The PS part mainly includes a configuration module, which is used for software-level configuration of the switch\u0026rsquo;s Gate Control List (GCL) and MAC forwarding table; the PL part mainly consists of the switch fabric and the gate control module, which are responsible for port forwarding and real-time control of traffic.\nAfter the data frame enters the PL from the input port, both ordinary traffic and key traffic will enter the switch fabric. The switch fabric will look up the corresponding output port based on the destination MAC address in the data frame, and then put the data frame into the priority queue in the gate module. The gate control module will control the gate state of each priority queue according to the pre-configured GCL, and then forward the data frame from the corresponding port. In the above process, the gate control module needs to obtain the globally synchronized time from the time synchronization module. The configuration module in the PS part mainly modifies the registers related to GCL (tsn_drivers\\gcl.c) and MAC forwarding table (tsn_drivers\\switch_rules.c) through the UIO driver and AXI4-Lite interface, thereby controlling the parameters in the switch fabric and gate module in the PL part.\nSource Code Description\r#\rTime Sync State Machine\r#\rThe main function for time synchronization is located in time_sync_main_loop.c. It is implemented based on IEEE 802.1AS 2020 standard. The following table introduces the relationship between the state machine code in the code and the state machine in the standard.\nCode Filename (.c/.h) Corresponding Section in 802.1AS-2020 site_sync_sync_sm 10.2.7 SiteSyncSync port_sync_sync_receive_sm 10.2.8 PortSyncSyncReceive clock_master_sync_send_sm 10.2.9 ClockMasterSyncSend clock_master_sync_receive_sm 10.2.11 ClockMasterSyncReceive port_sync_sync_send_sm 10.2.12 PortSyncSyncSend clock_slave_sync_sm 10.2.13 ClockSlaveSync port_announce_information_sm 10.3.12 PortAnnounceInformation port_state_selection_sm 10.3.13 PortStateSelection port_announce_information_ext_sm 10.3.14 PortAnnounceInformationExt port_announce_transmit_sm 10.3.16 PortAnnounceTransmit md_sync_receive_sm 11.2.14 MDSyncReceiveSM md_sync_send_sm 11.2.15 MDSyncSendSM md_pdelay_req_sm 11.2.19 MDPdelayReq md_pdelay_resp_sm 11.2.20 MDPdelayResp UIO addresses\r#\rThe UIO driver is mainly used to map logical addresses to physical addresses, thereby controlling the registers of modules such as TSU, RTC, GCL, etc. The driver code is located in the tsn_drivers folder. The correspondence between the register addresses in the software part and the hardware are described in the header files.\nFor example, in the tsn_drivers\\rtc.h file, the address of the RTC module is defined as follows:\n// define RTC address values #define RTC_CTRL 0x00000000 #define RTC_NULL_0x04 0x00000004 #define RTC_NULL_0x08 0x00000008 #define RTC_NULL_0x0C 0x0000000C #define RTC_TIME_SEC_H 0x00000010 #define RTC_TIME_SEC_L 0x00000014 #define RTC_TIME_NSC_H 0x00000018 #define RTC_TIME_NSC_L 0x0000001C #define RTC_PERIOD_H 0x00000020 #define RTC_PERIOD_L 0x00000024 #define RTC_ADJPER_H 0x00000028 #define RTC_ADJPER_L 0x0000002C #define RTC_ADJNUM 0x00000030 #define RTC_OFFSET_S_H 0x00000034 #define RTC_OFFSET_S_L 0x00000038 #define RTC_OFFSET_NS 0x0000003C MAC Forwarding\r#\rThe MAC address forwarding table configured in the switch_rules.c/h file actually operates the registers in swtich datapath and gate control list. These registers are in pairs, the first register represents the network byte order of the last 32 bits of the destination MAC address, and the second register represents the forwarding port.\nThe CaaS switch has 7 ports, of which 4 are external physical ports, and 3 are virtual ports inside the switch connecting PL and PS. The 3 internal virtual ports specifically include:\nTime Synchronization DMA: Used for transferring time synchronization data frames between PS and PL. PS ETH: Used for communication between PL\u0026rsquo;s physical network port and PS\u0026rsquo;s operating system (for example, using an SSH client to remotely log in and access the switch\u0026rsquo;s PS). PLC DMA: Used in CaaS to transfer input and output of control tasks. The following interface is provided to control the switch fabric\u0026rsquo;s forwarding rule:\nint push_switch_rule(char *mac_addr, int output_port) { /* Push a switch rule to the rule table mac_addr: 6 byte destination mac address. output_port: 0 -\u0026gt; to Port 0 1 -\u0026gt; to Port 1 2 -\u0026gt; to Port 2 3 -\u0026gt; to Port 3 4 -\u0026gt; to PLC DMA The switch rule for PTP frames are fixed in hardware, no need to specify explicitly. */ ... } Gate Control\r#\rTSN critical traffic data frames adopt the standard VLAN data frame format, and the priority is defined in the VLAN tag. VLAN refers to Virtual Local Area Network technology, defined in the 802.1Q standard. As shown in the figure below, the standard VLAN data frame contains a 4-byte VLAN tag, the TPID field represents the VLAN data frame type (0x8100), and the priority of critical traffic is defined in the PRI field, with a value range of [0, 7], corresponding to 8 priority queues. The output queue module identifies the priority of data frames based on the VLAN field in the critical data frame, and then places the data frame in the corresponding output port\u0026rsquo;s priority queue waiting for transmission.\nNotice: According to the IEEE 802.1Qbv standard, priority = 1 maps to priority 0; priority = 0 maps to priority 1; other priorities map to the corresponding queues. Therefore, normal traffic will default to entering priority queue 1 of the corresponding port.\nThe CaaS Switch\u0026rsquo;s gate control module implements the Time Aware Shaper defined by 802.1Qbv, which is used to execute hardware gate scheduling according to the TSN schedule table configured by PS, to ensure the deterministic transmission of critical traffic. Our GCL is represented by 9 bits, with the highest bit representing whether the Guardband is enabled, and the remaining 8 bits representing the gate switches. For example, 9'1_0000_0001 means that the Guardband is enabled, and only the gate switch of the first queue is open.\nNotice that the time unit of GCL in hardware is 2^11 ns, while the time unit in the configuration file issued is 2^14 ns.\nThe following interfaces in gcl.c/h are provided to get/set the hardware GCL (set the gate state and time interval seperately):\n/** * @description: This function is used to get gcl values of the port [portNumber]. * @param {uint16_t} portNumber port\u0026#39;s number. * @return {*} 0 by default. */ int get_gcl(uint16_t portNumber) { ... } /** * @description: This function is used to set GCL\u0026#39;s value, set port [portNumber] \u0026#39;s GCL[gcl_id] to [value]. * @param {uint16_t} portNumber port number, start from 0. * @param {uint16_t} gcl_id GCL index. * @param {uint16_t} value the GCL value to set. * @return {*} 0 by default. */ int set_gcl(uint16_t portNumber, uint16_t gcl_id, uint16_t value) { ... } /** * @description: This function is used to get all GCL time intervals of port [portNumber]. Consider we get time interval is x, the real time interval is (x * 2^8 * 8) nanoseconds. * @param {uint16_t} portNumber port number, start from 0. * @return {*} 0 by default. */ int get_gcl_time_interval(uint16_t portNumber) { ... } /** * @description: This function is used to set GCL\u0026#39;s time interval, set port [portNumber] \u0026#39;s GCL time interval[gcl_id] to [value]. * @param {uint16_t} portNumber port number, start from 0. * @param {uint16_t} gcl_id GCL index. * @param {uint16_t} value the GCL time interval x to set. The real time interval is (x * 2^8 * 8) nanoseconds. * @return {*} 0 by default. */ int set_gcl_time_interval(uint16_t portNumber, uint16_t gcl_id, uint16_t value) { ... } "},{"id":4,"href":"/docs/switch/hardware-build/","title":"Hardware Build","section":"ZIGGO Switch","content":"\rHardware Build\r#\rThis repo contains pre-build hardware \u0026amp; system rootfs to boot the Zynq AX7021 FPGA board from SD card.\nFile downloading\r#\rDownload the following file from this public link:\nBOOT.BIN boot.scr image.ub rootfs.tar.gz SD card partition\r#\rIn order to boot the CaaS Switch, you are supposed to have a micro SD card with \u0026gt;32GiB storage. Then use:\nsudo apt-get install gparted sudo gparted Parition it into two partition below\nBOOT: store boot files from petalinux\nFree space preceding (MiB): 4\nNew size (MiB): 500\nFile system: fat32\nLabel: BOOT\nROOTFS: store debian system rootfs\nFree space preceding (MiB): 0\nFree space following (MiB): 0\nFile system: ext4\nLabel: ROOTFS\nCopy files into SD card\r#\rMound SD card:\nsudo mount /dev/sda1 /media/alinx/BOOT/ sudo mount /dev/sda2 /media/alinx/ROOTFS/ Remove original files:\nsudo rm -rf /media/alinx/BOOT/* /media/alinx/ROOTFS/* Copy files:\nsudo cp BOOT.BIN boot.scr image.ub /media/alinx/BOOT sudo tar -zxvf rootfs.tar.gz -C /media/alinx/ROOTFS sudo cp -r ~/init_os.sh /media/alinx/ROOTFS/home/root/init_os.sh sync sudo chown root:root /media/alinx/ROOTFS sudo chmod 755 /media/alinx/ROOTFS Launch the switch\r#\r1. Launch the Board\r#\rPlug the SD card into FPGA board, turn the switch to SD card boot mode.\n2. Initialize PS\r#\rPlug in SD card, setup AX7021 board to boot on SD, power on.\nConnect a PC to the UART port of the board. We recommend using MobaXterm to connect the serial. Set up the Speed to 115200, Flow Control to None.\nThe default username and password are as follows:\nusername: \u0026#34;root\u0026#34; password: \u0026#34;root\u0026#34; Execute the initilization script to set up the linux environment.\nsh init_os.sh You can freely configure the host name, IP address, and MAC address, etc with the script, and can modify the script if needed.\n3. Connect to Internet\r#\rConnect the PC\u0026rsquo;s network port to the device\u0026rsquo;s PS network port (ETH0).\nSet up PC\u0026rsquo;s corresponding port to be in the same subnet with the device (i.e., 192.168.137.x).\nAfterwards, you can connect to the device through ssh and copy any Software files needed.\n4. Run the Software\r#\rPlease refer to the software part of this repo for further instructions.\n"},{"id":5,"href":"/docs/switch/software-build/","title":"Software Build","section":"ZIGGO Switch","content":"\rSoftware Build\r#\rThis repo contains source code to enable CaaS Switches\u0026rsquo; time synchronization logic and set up TSN GCL (gate control list), switch forwarding rules (including to dual-DMA).\nBuild\r#\rmkdir build cd build cmake .. make After successfully build, there should be two executables: \u0026ldquo;time_sync_app\u0026rdquo; \u0026amp; \u0026ldquo;switch_config\u0026rdquo;\nConfig\r#\rThe \u0026ldquo;config\u0026rdquo; directory contains topology \u0026amp; TSN/CaaS schedule results.\nThis document takes the following topology as an example to give example configurations:\n***-config.json: Mainly describes the network\u0026rsquo;s topology, including the type of each node (device or switch), MAC address, PTP port status, and other node information, topology information, and MAC forwarding table. Example and explanation are as follows (note that the comments in the file are just for explanation, do not have such comments in actual use):\n{ \u0026#34;nodes\u0026#34;: [ // Used to describe the information of each node in the network { \u0026#34;id\u0026#34;: 10, // Node ID, corresponding to \u0026#34;src\u0026#34; and \u0026#34;dst\u0026#34; below \u0026#34;type\u0026#34;: \u0026#34;switch\u0026#34;, // Node type, divided into \u0026#34;switch\u0026#34; and \u0026#34;device\u0026#34; \u0026#34;mac\u0026#34;: \u0026#34;00:0a:35:00:00:10\u0026#34;, // Node\u0026#39;s physical MAC address \u0026#34;ptp_ports\u0026#34;: [ 1, 0, 3, 0, 3 ] // Node\u0026#39;s PTP port status // The quintuple represents the clock status of [local, ETH1, ETH2, ETH3, ETH4] // Number meaning: (0: MASTER, 1: SLAVE, 2: PASSIVE, 3: DISABLED) // If local is 1, it means that the node is the master clock node; // If local is 0, it means that the node is the slave clock node. }, { \u0026#34;id\u0026#34;: 14, \u0026#34;type\u0026#34;: \u0026#34;device\u0026#34;, \u0026#34;mac\u0026#34;: \u0026#34;00:0a:35:00:00:14\u0026#34; }, { \u0026#34;id\u0026#34;: 15, \u0026#34;type\u0026#34;: \u0026#34;device\u0026#34;, \u0026#34;mac\u0026#34;: \u0026#34;00:0a:35:00:00:15\u0026#34; } ], \u0026#34;links\u0026#34;: [ // Topology information, composed of each link, each link is a directed edge { \u0026#34;id\u0026#34;: 0, // link ID \u0026#34;src\u0026#34;: 14, // Source node ID of the link \u0026#34;src_port\u0026#34;: 0, // Port number of the source node // port 0,1,2,3 correspond to ETH1,2,3,4 in reality \u0026#34;dst\u0026#34;: 10, // Destination node ID of the link \u0026#34;dst_port\u0026#34;: 0 // Port number of the destination node }, { \u0026#34;id\u0026#34;: 1, \u0026#34;src\u0026#34;: 10, \u0026#34;src_port\u0026#34;: 0, \u0026#34;dst\u0026#34;: 14, \u0026#34;dst_port\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: 2, \u0026#34;src\u0026#34;: 10, \u0026#34;src_port\u0026#34;: 2, \u0026#34;dst\u0026#34;: 15, \u0026#34;dst_port\u0026#34;: 0 }, { \u0026#34;id\u0026#34;: 3, \u0026#34;src\u0026#34;: 15, \u0026#34;src_port\u0026#34;: 0, \u0026#34;dst\u0026#34;: 10, \u0026#34;dst_port\u0026#34;: 2 } ], \u0026#34;fwd\u0026#34;: [ // Forwarding table, which can be generated by the scheduling algorithm in CNC // In the forwarding table where you send to yourself, the port is 4 (caas) / 5 (PS ETH for new hardware) { \u0026#34;src\u0026#34;: 10, // Current node ID \u0026#34;dst\u0026#34;: 14, // Output port number \u0026#34;id\u0026#34;: 0, // Entry ID \u0026#34;src_port\u0026#34;: 0 // Destination node ID }, { \u0026#34;src\u0026#34;: 14, \u0026#34;dst\u0026#34;: 10, \u0026#34;id\u0026#34;: 1, \u0026#34;src_port\u0026#34;: 0 }, { \u0026#34;src\u0026#34;: 10, \u0026#34;dst\u0026#34;: 15, \u0026#34;id\u0026#34;: 2, \u0026#34;src_port\u0026#34;: 2 }, { \u0026#34;src\u0026#34;: 15, \u0026#34;dst\u0026#34;: 10, \u0026#34;id\u0026#34;: 3, \u0026#34;src_port\u0026#34;: 0 } ] } If the master clock selection algorithm is used, only the configuration of nodes is different, and the rest are the same. The newly added externalPortConfigurationEnabled and system_identity fields are optional items, as shown in the example below:\n{ \u0026#34;nodes\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;switch\u0026#34;, \u0026#34;id\u0026#34;: 0, \u0026#34;mac\u0026#34;: \u0026#34;00:00:00:00:02:01\u0026#34;, \u0026#34;ptp_ports\u0026#34;: [0,0,0,0,0], \u0026#34;externalPortConfigurationEnabled\u0026#34;: 1, // 1: Manually configure the master-slave relationship, 0: Configure the master-slave relationship through the master clock selection algorithm, if this item is not written, the default is 0, that is, configure the master-slave relationship through the master clock selection \u0026#34;system_identity\u0026#34;: { // Clock node\u0026#39;s clock parameters, used for comparison in the master clock selection algorithm, if this item is not written, the default is the configuration written below \u0026#34;priority1\u0026#34;: 254, // First priority \u0026#34;clockClass\u0026#34;: 248, // Clock level \u0026#34;clockAccuracy\u0026#34;: 254, // Clock accuracy \u0026#34;offsetScaledLogVariance\u0026#34;: 17258, // Clock variance \u0026#34;priority2\u0026#34;: 247, // Second priority \u0026#34;clock_identity\u0026#34;: [0,0,0,0,0,0,0,0] // Clock identifier, different clocks should have different parameters } } ], \u0026#34;links\u0026#34;: [ ], \u0026#34;fwd\u0026#34;: [ ] } ***-schedule.json: schedule file, contains each links\u0026rsquo; schedule time interval \u0026amp; each CaaS switch\u0026rsquo;s computation time interval.\n[ {// All switches need to be listed to facilitate software recognition of configuration information \u0026#34;type\u0026#34;: \u0026#34;switch\u0026#34;, // Node type, divided into \u0026#34;switch\u0026#34; and \u0026#34;device\u0026#34; \u0026#34;id\u0026#34;: 10, // Node ID, corresponding to \u0026#34;src\u0026#34; and \u0026#34;dst\u0026#34; below \u0026#34;mac\u0026#34;: \u0026#34;00:0a:35:00:00:10\u0026#34;, // Node\u0026#39;s physical MAC address \u0026#34;schedule\u0026#34;: [ ] // Can be empty, not used }, { \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, // Type is link, used to describe scheduling information \u0026#34;from\u0026#34;: 14, // Source node ID of the link \u0026#34;to\u0026#34;: 10, // Destination node ID of the link \u0026#34;from_port\u0026#34;: 0, // Output port number \u0026#34;id\u0026#34;: 3, \u0026#34;schedule\u0026#34;: [ // Scheduling information { \u0026#34;period\u0026#34;: 2048, // Scheduling cycle \u0026#34;start\u0026#34;: 0, // Start time relative to the entire cycle \u0026#34;end\u0026#34;: 5, // End time relative to the entire cycle \u0026#34;job_id\u0026#34;: 0, // Job ID in CaaS, if not needed, it can be omitted \u0026#34;flow_id\u0026#34;: 0 // Data stream ID } ] }, { \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, \u0026#34;from\u0026#34;: 10, \u0026#34;to\u0026#34;: 15, \u0026#34;from_port\u0026#34;: 2, \u0026#34;id\u0026#34;: 0, \u0026#34;schedule\u0026#34;: [ { \u0026#34;period\u0026#34;: 2048, \u0026#34;start\u0026#34;: 1, \u0026#34;end\u0026#34;: 6, \u0026#34;job_id\u0026#34;: 0, \u0026#34;flow_id\u0026#34;: 0, \u0026#34;pkt_size\u0026#34;: 1500 // Packet length, if not written, the default is 1500B, this item only exists on the path from Device to Switch } ] } ] Run\r#\rCopy topology \u0026amp; schedule file to build dir: cp [topology name]-config.json build/config.json cp [topology name]-schedule.json build/schedule.json Start time synchronization and initialize GCL and MAC forwarding table according to the configuration (it is recommended to run the time synchronization program on a core (with taskset -c 1 command) to prevent kernel errors and crashes due to time synchronization): taskset -c 1 ./time_sync The log output level is categorized into three types: WARN, INFO, and TRACE. The default log output level is TRACE (the most comprehensive output information, including DEBUG logs). The log output level can be set with the -l parameter: taskset -c 1 ./time_sync -l w # WARN level taskset -c 1 ./time_sync -l i # INFO level taskset -c 1 ./time_sync -l t # TRACE level Notice that the time sync logic is supposed to run indefinitely as the node should sync to its neighbors again and again.\nUpdate GCL \u0026amp; switch forwarding rules: ./switch_config "},{"id":6,"href":"/docs/tsnperf/","title":"ZIGGO TSNPerf","section":"Docs","content":"1111\n"},{"id":7,"href":"/docs/switch/contributing/","title":"How to contribute","section":"ZIGGO Switch","content":"\rHow to contribute\r#\rReporting a bug\r#\rFile bugs in the Github Issue Tracker. Please follow these guidelines:\nSearch existing issues first, make sure yours hasn\u0026rsquo;t already been reported. Consider enabling debug mode so that you can provide as much details as possible when reporting the issue. Stay on topic, but describe the issue in detail so that others can reproduce it. Provide a screenshot if possible. A screenshot showing the problem is often more useful than a paragraph describing it. The development team will only answer questions on github issues and reject other forms of questions.\nContributing to ZIGGO\u0026rsquo;s code\r#\rIf you want to start contributing to the project\u0026rsquo;s code, please follow these guidelines before creating a pull request:\nThe top post of the pull request should contain a full, self-contained explanation of the feature: what it does, how it does it, with examples of usage and screenshots. Also explain why you want to add this - what problem does it solve. Do not simply add a text Implement feature #4345 , because the information there will most likely be outdated or confusing (multiple discussions and opinions). The pull request needs to be self-contained. Bug fixes are always welcome. A good way to easily start contributing is to pick and work on a good first issue. We try to make these issues as clear as possible and provide basic info on how the code should be changed, and if something is unclear feel free to ask for more information on the issue. Before adding a new feature, ask about it in the Github Issue Tracker , or check if existing discussions exist to make sure the new functionality is desired. Pull requests that make many changes using an automated tool, like for spell fixing, styling, etc. will not be accepted. Pull requests that address multiple issues will most likely stall and eventually be closed. This is because we might be fine with one of the changes but not with others and untangling that kind of pull request is too much hassle both for maintainers and the person who submitted it. So most of the time someone gives up and the PR gets closed. So please keep the pull request focused on one issue. Do not mark your reviewer\u0026rsquo;s comments as \u0026ldquo;resolved\u0026rdquo;. If you do that, the comments will be hidden and the reviewer will not know what are the pending issues in the pull request. Only the reviewer should resolve the comments. "},{"id":8,"href":"/docs/switch/","title":"ZIGGO Switch","section":"Docs","content":" ZIGGO CaaS Switch: A flexible, standard-compliant, and control-function-virtualized TSN switch platform\r#\rTable of Contents\r#\rZIGGO CaaS Switch: A flexible, standard-compliant, and control-function-virtualized TSN switch platform Table of Contents Introduction ZIGGO Open Platform Demo Features Read before start Getting Started System Design Demo APP Tutorial License and Citation TODO List Contributing Introduction\r#\rZIGGO is a flexible, standard-compliant, and control-function-virtualized TSN switch platform ready for industrial control, automotive electronics, and other time-sensitive applications.\nThis is the document for the ZIGGO CaaS Switch. (We also offer ZIGGO-Device that comply with the IEEE 802.1 TSN standard.)\nZIGGO Open Platform\r#\rThe construction of the ZIGGO Open Platform consists of three levels: network device, management tools, and a Demo App. More details in ZIGGO-Device.\nDemo\r#\rWe provide a demonstration video of the TSN switch. It demonstrates the superior performance of the ZIGGO-CaaS-Switch compared to the normal switch.\nThe left side of the picture is the ZYNQ development board we use, and the right side is the TSN display board we built.\nClick the pic to watch the video! Or just click here.\nFeatures\r#\rZIGGO supports the simultaneous transmission of both Information Technology (IT) and Operation Technology (OT) data traffic with QoS guarantee.\nZIGGO complies with IEEE standards 802.1AS, Qav, Qbv, and Qcc.\nZIGGO provides Real-time and Deterministic Ethernet transport\nZIGGO achieve Zero Packet Loss , Microsecond-level Latency with Nanosecond-level Jitter Gate Ability.\nZIGGO guarantee Gigabit Throughput.\nZIGGO provide gate accuracy applicable to All Ethernet Frame Sizes.\nRead before start\r#\rGetting started with ZIGGO-CaaS-Switch/ZIGGO-Device is a pretty hard task. Users/developers need to have sufficient basic knowledge and be prepare to for a long periond of learning and debugging.\nPlease refer to basic_knowledge.md to check if you have ability to use ZIGGO competently.\nGetting Started\r#\rPlease refer to required.md to get prepared.\nPlease refer to hardware-build.md for the build hardware for ZIGGO Evaluation Toolkit and software-build.md to run time synchronization logic and set up TSN GCL .\nSystem Design\r#\rZIGGO is implemented on ZYNQ-7000 SoC and exploits ZYNQ\u0026rsquo;s both hardware and software programmability.\nWe also provide more in-depth documentation explaining specific design principles for ZIGGO CaaS Switch.\nDemo APP Tutorial\r#\rWe also provide a testbed build document(in ZIGGO Device) that allows you to build a real-time Ethernet system using the ZIGGO swtich and ZIGGO Device.\nThrough this platform, we can measure the delay and jitter of TSN time-critcial traffic, the switch\u0026rsquo;s gating capability, bandwidth guarantee and gating accuracy.\nReplacing ZIGGO CaaS switches with commercial TSN switches can also test its above capabilities.\nLicense and Citation\r#\rZIGGO is released under a MIT license.\nPlease consider citing our papers if the project helps your research with the following BibTex:\n@inproceedings{caas, author={Yang, Zheng and Zhao, Yi and Dang, Fan and He, Xiaowu and Wu, Jiahang and Cao, Hao and Wang, Zeyu and Liu, Yunhao}, booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}, title={CaaS: Enabling Control-as-a-Service for Time-Sensitive Networking}, year={2023}, pages={1-10}, doi={10.1109/INFOCOM53939.2023.10228980} } @inproceedings{etsn, author={Zhao, Yi and Yang, Zheng and He, Xiaowu and Wu, Jiahang and Cao, Hao and Dong, Liang and Dang, Fan and Liu, Yunhao}, booktitle={IEEE ICDCS 2022 - IEEE International Conference on Distributed Computing Systems}, title={E-TSN: Enabling Event-triggered Critical Traffic in Time-Sensitive Networking for Industrial Applications}, year={2022}, volume={}, number={}, pages={691-701}, doi={10.1109/ICDCS54860.2022.00072}} TODO List\r#\rZIGGO CaaS Switch Release\nZIGGO Evaluation Toolkit Release\nZIGGO Evaluation Toolkit Source Code\nTutorial for build a testbed\nTest Case for TSN\nWe will expand each test in the tutorial to multiple test cases to cover different edge cases and comprehensively test the performance of TSN switches.\nSupport Device List At present, we have only tested our own Ziggo switches and are testing other commercial switches (such as Huawei ,H3C and NXP). We expect to maintain a list of test results in the future.\nContributing\r#\rPlease see the guide for information on how to ask for help or contribute to the development of ZIGGO!\nThe development team will only answer questions on github issues and reject other forms of questions.\n"},{"id":9,"href":"/docs/tsnperf/configuration/","title":"TSNPerf 使用指南","section":"ZIGGO TSNPerf","content":"\rTSNPerf 使用指南\r#\r硬件准备\r#\r准备两台机器，一台作为发送方Publisher，一台作为接收方Recorder，每一台上的硬件需满足以下条件\nIntel 八代及以上的处理器 Intel I210 / I225-LM / I225-LM 网卡 注意：Publisher上需要安装两个网卡，一个负责时间同步，一个负责发送数据包。Recorder上只要安装一个网卡，该网卡同时负责时间同步和数据接收。\n系统准备\r#\r在发送发和接收方的机器上，安装以下系统与软件依赖\n安装 Ubuntu 22.04 安装 Intel ECI https://eci.intel.com/docs/3.1/getstarted/requirements.html 安装linuxPTP，参考下一章节 安装依赖 sudo apt-get install libconfig-dev libpcap-dev 在进行实验前，请将待测设备与Publisher、Recorder按照合适的方式连接，并开机启动。\n启用时间同步\r#\r安装linuxPTP\r#\r1. 安装网卡驱动\r#\r下载驱动，参照压缩包中 README 的教程编译安装驱动。\n使用下面的命令检查安装（enp3s0为网卡名）。如安装成功，应有下图红框中的输出（表示网卡具有硬件时间戳功能）：\nethtool -T enp3s0 2. 安装linuxPTP\r#\r按照以下命令安装linuxPTP\ngit clone http://git.code.sf.net/p/linuxptp/code linuxptp cd linuxptp/ make sudo make install 同步Publisher与Recorder的网卡时钟\r#\r用以下命令启动Publisher和Recorder的时间同步网卡。enp3s0为网卡名，\u0026lt;configuration\u0026gt;.cfg为配置文件，可参考linuxptp/configs/中的文件，并结合需求写该配置。\nsudo ptp4l -i enp3s0 -f \u0026lt;configuration\u0026gt;.cfg -m -l 6 --socket_priority 7 注意：如有需要，请同时启动待测设备的时间同步功能\n同步网卡时钟与系统时钟\r#\rPublisher和Recorder的两张网卡同步完成后，再将网卡时钟与系统时钟同步上。\nsudo phc2sys -c CLOCK_REALTIME -s enp3s0 -O -0 -m -l 6 由于Publisher上时间同步与数据包发送使用了不同的网卡，请将发送数据包的网卡上的时钟也与系统同步上\nsudo phc2sys -s CLOCK_REALTIME -c enp1s0 -O -0 -m -l 6 优化网络栈\r#\r将关键流量绑定到 Queue 0\r#\r在Publisher和Recorder上执行以下命令，将关键流量绑定到 Queue 0 上。\nsudo ethtool --config-ntuple enp1s0 delete 15 sudo ethtool --config-ntuple enp1s0 flow-type ether proto 0xb62c loc 15 action 0 sudo ethtool -X enp1s0 equal 2 注意：\nenp1s0代表Publisher上负责发送数据包的网卡，或者Recorder上的网卡。 第2行命令中的0xb62c代表关键流量的类型，请保持该值与下一章节中ethertype一致。 绑定 Queue 0 中断到隔离核心，并设置优先级\r#\r在Publisher和Recorder上执行以下命令。\n绑定到隔离核心\ncat /proc/interrupts | grep TxRx-0 sudo sh -c \u0026#34;echo 2 \u0026gt; /proc/irq/131/smp_affinity\u0026#34; 设置优先级\nps -ae | grep 131 sudo chrt -fp 95 开启 LaunchTime 功能\r#\r在Publisher上开启 LaunchTime 功能\nsudo tc qdisc add dev enp1s0 handle 8001: parent root mqprio num_tc 4 map 0 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 queues 1@0 1@1 1@2 1@3 hw 0 sudo tc qdisc replace dev enp1s0 parent 8001:1 etf offload clockid CLOCK_TAI delta 500000 修改配置文件\r#\r根据测试需求，修改配置文件，以下是一个示例配置 default.ini\nmode: 0 tx-mode: 0 verbose: true use-ziggo-analysis: false pcap-filename: \u0026#34;/home/i210/launchtimedemo/captured_10w_1500Byte.pcap\u0026#34; interface: \u0026#34;enp1s0\u0026#34; smac: \u0026#34;00:1b:21:77:ac:ae\u0026#34; dmac: \u0026#34;00:1b:21:76:ae:75\u0026#34; ethertype: 0xb62c socket-priority: 0 vlan-priority: 0 offset: 150000 early-offset: 300000 use-launchtime: true basetime: 1684559640000000100L packet-size: 1500 packets-to-send: 100000 interval: 1000000 use-udp: true sip: \u0026#34;192.168.16.10\u0026#34; dip: \u0026#34;192.168.16.11\u0026#34; sport: 10000 dport: 10000 下面是每个配置参数的含义：\nmode：运行模式，INT类型。0 表示发送，1 表示接收。目前仅支持发送功能。 tx-mode：发送模式，INT类型。0 表示 完全按照pcap文件中的时间戳，重放数据包；1 表示 按顺序每周期发送一个pcap文件中的数据包；2 表示 每个周期发送一个构造的数据包； 3 表示 全力向网络中发送数据包（用于带宽保证测试）。 verbose：是否输出额外的信息，BOOL类型。主要包括每发出一个数据包的 序号 和 tx 时间戳。 use-ziggo-analysis：是否使用ZIGGO测试模式，BOOL类型。使用 ZIGGO 测试，会将 tx 时间戳 打印在ethernet数据包的第20个字节开始（从0编号）的8个字节；否则会使用 Intel 测试模式，会将 tx 时间戳 打印在第26个字节开始的8个字节； pcap-filename：需要重放的 pcap 文件，STRING类型。当tx-mode=0/1时有效。 interface：网口名字，STRING类型。 smac，dmac：源/目的mac地址，STRING类型。 ethertype：数据包类型，INT类型。当tx-mode=2/3时有效。 socket-priority, vlan-priority：数据包类型，INT类型。vlan相关参数，默认值为0。当tx-mode=2/3时有效。 offset：提前调用send的时长，单位ns，INT类型。当use-launchtime为true时有效。 early-offset：给网络栈处理预留的时长，单位ns，INT类型。当use-launchtime为true时有效。 use-launchtime：是否使用 launchtime，BOOL类型。只有当使用 launchtime 时，数据包的tx时间戳精度才是亚微秒级别。注意：目前，当且仅当仅支持tx-mode=2时，可将use-launchtime设置为false。 basetime：INT64类型。整个网络设备共享的基准时间，用于调整数据包的发送时间，与交换机的门控配合。 packet-size：数据包大小，INT类型。单位Byte。当tx-mode=2/3时有效。 packets-to-send：发送数据包的数量，INT类型。 interval：一个周期的时长，INT类型。单位ns。当tx-mode=1/2时有效。 use-udp：是否使用udp填充以太网报文，BOOL类型。注意：目前，当且仅当仅支持tx-mode=2/3时，可将use-udp设置为true。 sip：原ip地址，STRING类型。注意ip地址是字符串，需要加上引号。 dip：目的ip地址，STRING类型。注意ip地址是字符串，需要加上引号。 sport：原端口号，INT类型。 dport：目的端口号，INT类型。 在Publisher启动接收程序\r#\r接收程序可以使用 Intel 的 txrx-tsn 中的接收模式，配合 use-ziggo-analysis = false，其打出来的 log 可以直接拿到数据包的发送时间戳和接收时间戳。 但是实验中发现，该程序与iperf打背景流量冲突，可以考虑用tcpdump抓包，并保存高精度的 rx 时间戳，tx时间戳可以从发送端的程序输出获得，从而进行分析。\nsudo tcpdump -i \u0026lt;网口\u0026gt; ether proto 0xb62c -j adapter_unsynced --time-stamp-precision=nano -s 0 -tttt -w \u0026lt;存储的pcap文件\u0026gt; 在Recorder启动发送程序\r#\rcd /path/to/launchtimedemo make sudo taskset -c 1 chrt -f 92 ./pcap_replay -c ~/launchtimedemo/config.ini "},{"id":10,"href":"/docs/device/basic_knowledge/","title":"基础知识列表","section":"ZIGGO Device","content":"\r基础知识列表\r#\r我们再次重申，使用或者开发ZIGGO-CaaS-Switch或者ZIGGO-Device是有较高门槛的，您必须掌握一定的软硬件知识。如果您之间没有任何的硬件开发经验，您可能需要慎重考虑自己是否适合这个项目，因为学习这些知识是非常花时间的。\n学习使用ZIGGO项目比学习FPGA、MCU、ARM等传统开发工具要求更高，想如臂指使一般地使用ZIGGO设备也不是一蹴而就的事情。\n如果您希望使用ZIGGO项目，您至少应该有以下知识：\n硬件知识\r#\r计算机组成原理 C语言 数字电路基础 Verilog、VHDL语言 良好的英语阅读基础 软件知识\r#\r计算机组成原理 计算机网络 计算机操作系统 C、C++语言 tcl脚本 良好的英语阅读基础 "},{"id":11,"href":"/docs/device/require/","title":"准备清单","section":"ZIGGO Device","content":"\r准备清单\r#\r软件环境清单\r#\rVivada 开发环境\r#\rVivado 是开发板的硬件开发程序。\nVivado 软件的 Xilinx 官方下载地址： http://china.xilinx.com/support/download.html\n本开发团队使用的是Vivado 2020.1版本，我们建议ZIGGO项目使用者也使用相同版本以免一些不必要的麻烦。\n具体的安装流程可以参照 ZYNQ应用教程 第三章Vivado开发环境 进行安装。\nMobaXterm\r#\rMobaXterm是串口连接工具，我们在hardware_build.md中“Launch the Board and log in”这一章节进行了使用介绍。\nMobaXterm的官方下载地址：MobaXterm Xserver with SSH, telnet, RDP, VNC and X11 - Download (mobatek.net)\n硬件准备清单\r#\rAlinx7021开发板\r#\rSD启动卡\r#\rSD启动卡中存储着操作系统，建议容量 $\\geq$ 32G。\n必要的线材\r#\r包括网线、数据线等。\n普通交换机\r#\r如果使用的开发板数量较多，可以考虑将所有开发板连接到普通交换机再通过普通交换机连接至PC（通过网线）进行管理和使用。\n"},{"id":12,"href":"/docs/device/getting-started/","title":"Getting start","section":"ZIGGO Device","content":"\rGetting start\r#\rTable of Content\r#\rGetting start Table of Content Folder Structure Introduction to git branches Building and Starting the TSNPerf Folder Structure\r#\r. ├── docs //documents for ZIGGO ├── figs //figures for docs ├── hardware //hareware code, for PL part(pkt_gen, time sync in FPGA) ├── readme.md //readme for whole project ├── software //software code, for ps part(pkt_gen, time sync) └── testbed-build //code for building testbed Introduction to git branches\r#\rBranch Name Basic Function Packet Resize Online Analyze Offline Analyze main √ √ packet_resize √ √ √ offline_analyze √ √ The branches of TSNPerf submitted this time can be divided into three categories. But the main branch is inconvenient to use in two ways. In order to solve the above two limitations, we have also submitted two patch branches.\nThe main branch contains basic time synchronization and pkt_gen functions, as well as online analysis functions. However, its limitation is that it can only send Ethernet of MTU (1500B) size, and due to the limited performance of the zynq development board, online analysis may cause packet loss. packet_resize branch can send Ethernet frames of any size (64B-1500B). offline_analyse branch can forward the data packets to the PC for offline analysis to avoid packet loss. Note1: When switch to \u0026lsquo;packet_resize\u0026rsquo; branch, you also need to modify the software/config/flow.json\n{\r\u0026#34;job_id\u0026#34;: 0, \u0026#34;flow_id\u0026#34;: 0,\r\u0026#34;src\u0026#34;: 1,\r\u0026#34;dst\u0026#34;: 2,\r\u0026#34;period\u0026#34;: 2048,\r\u0026#34;MD\u0026#34;: 1024,\r\u0026#34;packet_size\u0026#34;: 750 // add this attribute }, Note2: When switch to \u0026lsquo;offline_anaylze\u0026rsquo; branch, you need to link device to anthor PC (linux) by wire in ETH1.\nBuilding and Starting the TSNPerf\r#\rSet up the FPGA board and initialize PS system\nCompile software code, run the time synchronization \u0026amp; pkt _gen_app\n"},{"id":13,"href":"/docs/device/system-design/","title":"System Design","section":"ZIGGO Device","content":"\rSystem Design\r#\rZIGGO is implemented on ZYNQ-7000 SoC and exploits ZYNQ\u0026rsquo;s both hardware and software programmability.\nTable of Content\r#\rSystem Design Table of Content Evaluation Method Test data frame data structure Module Design Correspondence between software and hardware registers Offline Analysis and Design Evaluation Method\r#\rThe ZIGGO Evaluation Toolkit is used to send and receive test data frames for the purpose of ensuring accurate delay calculations.\nTo achieve this accuracy, the toolkit needs to synchronize its time with the TSN switch and record a global hardware timestamp in the test data frames.\nAs shown in the diagram below, in a data flow link with $n$ network nodes, Node $1$ serves as the source node to send test data frames, Node $n$ acts as the destination node to receive test data frames, and all the intermediate nodes (Node $2$ to Node $n-1$) are TSN switches.\nWhen a test data frame is sent from the source node, the hardware records the timestamp $t^+$ of when the frame was sent in the data frame. Finally, when the data frame arrives at the destination node, the destination node records the timestamp $t^-$ of when the frame entered that node. By calculating $(t^−−t^+)$, we can determine the end-to-end latency of the data frame.\nIn this doc, $t^{dir}$ is used to represent the timestamp of data frames entering and leaving the hardware. Here, \u0026ldquo;dir\u0026rdquo; represents the direction of data frames entering or leaving the nodes, with \u0026quot; + \u0026quot; indicating data frames leaving the node and \u0026quot; − \u0026quot; indicating data frames entering the node.\nTest data frame data structure\r#\rIn order to analyze the end-to-end latency of each data frame on complex network topologies, we have established specific guidelines for the content of data segments in the Ethernet frame structure. The structure of time-sensitive networking test data frames is as shown in the following table.\nield Byte Length Description Destination Address 6 Destination node MAC address Source Address 6 Source node MAC address VLAN Tag 4 Divided into four fields: VLAN Data Frame Type (0x8100), Priority, Canonical Format Indicator, and VLAN Number Data Frame Type 2 Used to identify test data frames, set to 0x66ab Reserved Bits 2 $t^+$ (TX Timestamp) 8 Timestamp when transmitted from the source node $t^-$ (RX Timestamp) 8 Timestamp when received by the destination node Data Stream ID (SEQ_ID) 2 Unique identifier for each data stream Data Frame ID (PKT_ID) 4 Sequence number of data frames within each data stream, starting from 0 Module Design\r#\rThe overall module design for ZIGGO Evaluation Toolkit is depicted in the following diagram. The Time Synchronization module used by the Toolkit shares a design that is nearly identical to the Switch, and the diagram does not provide extensive details on the time synchronization-related modules.\nThe configuration module in the PS section (software/pkt_gen_main.cpp) communicates with the Data Frame Generation module by using the UIO driver and AXI4-Lite interface to send the transmission rules for the test data frames to relevant registers in the Data Frame Generation module (specifically, hardware/IP_repo/pkt_gen_controller_v1.1/pkt_gen_controller_1.1/hdl/pkt_gen_controller_v1_1.v is responsible for configuration, and hardwire/HDL/axi_packet_generator.v is responsible for generating and transmitting the data frames) using the global synchronized time as a reference for timing.\nThe Timestamp Marking module (hardwire/HDL/hw_timestamp/tsu/tsu_axis_tx.v and hardwire/HDL/hw_timestamp/tsu/tsu_axis_rx.v) is located at the send and receive ports of the Ziggo-Evaluation-Toolkit. It is used to record the hardware timestamps of sent and received data frames within the test data frames.\nThe received data frames are ultimately uploaded to the PS section via a DMA channel (in our design, we currently use a time-synchronized DMA channel for transmitting test data frames) for statistical analysis (handled in the process_critical_frame function in software/pkt_gen_control/pkt_gen.c).\nThe maximum frame transmission period (superperiod parameter in pkt_gen_main.cpp) refers to the upper limit on the periodicity for sending data streams from the toolkit. Within this maximum frame transmission period, the Data Frame Generation module allocates a maximum of 32 time slots for all data streams. Each time slot is used to send all the data frames for the corresponding data stream at the specified transmission time.\nAs illustrated in the diagram, the evaluation toolkit sends two types of data streams: Stream 𝐹1 with a transmission period of 2𝑇 and Stream 𝐹2 with a transmission period of 4𝑇. The maximum frame transmission period for the evaluation toolkit is 6𝑇. In this scenario, the Data Frame Generation module sets up 5 time slots within the maximum frame transmission period and sends test data frames in sequential order according to the specified time slots.\nUsers can configure the Ziggo-Evaluation-Toolkit to generate multiple data streams, and the configuration parameters for each data stream are as follows, with clear explanations provided in the comments of the pkt_gen.h header file:\n// pkt_gen_app\\pkt_gen_control\\pkt_gen.h /** * @brief Set the pkt gen slot object * * @param slot_id 0~31, indicate a slot in pkt_gen IP core * IMPORTANT: tx_offset for each slot id should be monotonic increasing * @param seq_id sequence ID that uniquely identify a stream * To further utilize this field, it may be {job_id: 8bit, flow_id: 8bit} * @param pkt_number number of packets in one sent, should be 1 * @param pkt_id_start start of pkt_id * @param pkt_id_update update of next pkt_id * @param tx_offset transmission time inside a period, in ns * @param src_mac 6Byte array * @param dest_mac 6Byte array * @return int */ int set_pkt_gen_slot (int slot_id, uint16_t seq_id, uint16_t pkt_number, uint32_t pkt_id_start, uint32_t pkt_id_update, int64_t tx_offset, uint8_t *src_mac, uint8_t *dest_mac); The Timestamp Marking Module will write the timestamp of the current synchronized clock into the outgoing data frame just before it is transmitted by the MAC controller of the toolkit. When the MAC controller in the PL (Programmable Logic) receives the first bit of data in the frame from the physical link, the Timestamp Marking Module records the timestamp of the current synchronized clock. It subsequently writes this timestamp into the data frame during the packet reception process and hands it over to the statistical analysis module in the PS (Processing System) section for further analysis and processing.\nCorrespondence between software and hardware registers\r#\rThe correspondence in terms of time synchronization is the same as in the Ziggo TSN Switch.\nIn this section, we will primarily focus on the correspondence related to data frame transmission configuration. The software registers\u0026rsquo; addresses can be found in software/pkt_gen_control/pkt_gen.c.\n// define global register address #define GLOBAL_PERIOD_NS 0x00000000 #define GLOBAL_OFFSET_NS 0x00000008 #define SEQ_VALID 0x00000010 #define SEQ_ENABLE_VLAN 0x00000014 #define WRITE_LOCK 0x00000018 // define write lock value #define NOT_WRITING 0x00000001 #define WRITING 0x00000000 // define address pointer for sequences #define SEQ_CONTENT 0x0000001c The hardware registers are located in tsn_device\\IP_repo\\pkt_gen_controller_v1.1\\pkt_gen_controller_1.1\\hdl\\pkt_gen_controller_v1_1_S00_AXI.v.\nif (slv_reg_wren) begin case ( axi_awaddr[ADDR_LSB+OPT_MEM_ADDR_BITS:ADDR_LSB] ) 9\u0026#39;h000: for ( byte_index = 0; byte_index \u0026lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 0 slv_reg0[(byte_index*8) +: 8] \u0026lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 9\u0026#39;h001: for ( byte_index = 0; byte_index \u0026lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 1 slv_reg1[(byte_index*8) +: 8] \u0026lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 9\u0026#39;h002: for ( byte_index = 0; byte_index \u0026lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 2 slv_reg2[(byte_index*8) +: 8] \u0026lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end ... In this context, the hardware registers with names starting with \u0026ldquo;slv_reg\u0026rdquo; are each 32 bits wide, and their addresses are spaced by 4 bytes. So, for example, if the software-level address is $0x00000018$, you need to divide this address by 4, which results in $ 24/4 = 6$. Therefore, this address corresponds to the hardware register \u0026ldquo;slv_reg6\u0026rdquo; in the hardware module. This helps establish the correspondence between software and hardware registers.\nOffline Analysis and Design\r#\rIn the offline_analyze branch of the Toolkit, we perform packet capture and analysis by forwarding packets from the Toolkit to a powerful desktop computer after the timestamps the incoming packets. This method ensures that even at gigabit speeds, no packets are dropped, and there are relatively minor hardware changes.\nOne aspect of the modification is that timestamps are not added to the data frames when they are transmitted from the toolkit. This can be seen in the HDL/trimode_mac/simple_mac_no_shared.v file where the \u0026ldquo;tsu_axis_tx\u0026rdquo; section is commented out.\nAnother aspect is the modification of the \u0026ldquo;frame_type\u0026rdquo; to change the direction of data frames. Previously, both PTP frames and test data frames were uploaded to the PS section via time-synchronized DMA, with IT traffic uploaded to PS_ETH. This split the traffic into two directions (\u0026ldquo;axis_switch_1_2\u0026rdquo;). Now, it has been split into three directions (\u0026ldquo;axis_switch_1_3\u0026rdquo;). For offline analysis, only PTP frames need to be transferred to the PS, while test data frames need to be separated and sent out from ETH2. This requires a finer-grained separation of traffic.\n// separate IT frames, ptp frames and critical frames axis_switch_1_3 axis_switch_1_3_inst ( .aclk(rx_fifo_clock), // input wire aclk .aresetn(rx_fifo_resetn), // input wire aresetn .s_axis_tvalid(rx_axis_fifo_tvalid_8), // input wire [0 : 0] s_axis_tvalid .s_axis_tready(rx_axis_fifo_tready_8), // output wire [0 : 0] s_axis_tready .s_axis_tdata(rx_axis_fifo_tdata_8), // input wire [7 : 0] s_axis_tdata .s_axis_tlast(rx_axis_fifo_tlast_8), // input wire [0 : 0] s_axis_tlast .s_axis_tdest(frame_type), // input wire [1 : 0] s_axis_tdest .m_axis_tvalid({rx_axis_it_fifo_tvalid, rx_axis_ptp_fifo_tvalid, tx_axis_fifo_legacy_tvalid[1]}), // output wire [2 : 0] m_axis_tvalid .m_axis_tready({rx_axis_it_fifo_tready, rx_axis_ptp_fifo_tready, tx_axis_fifo_legacy_tready[1]}), // input wire [2 : 0] m_axis_tready .m_axis_tdata({rx_axis_it_fifo_tdata, rx_axis_ptp_fifo_tdata, tx_axis_fifo_legacy_tdata[1]}), // output wire [23 : 0] m_axis_tdata .m_axis_tlast({rx_axis_it_fifo_tlast, rx_axis_ptp_fifo_tlast, tx_axis_fifo_legacy_tlast[1]}), // output wire [2 : 0] m_axis_tlast .m_axis_tdest(), // output wire [5 : 0] m_axis_tdest .s_decode_err() // output wire [0 : 0] s_decode_err ); The tx_axis_fifo_legacy_tdata[1] indicates sending from ETH2.\n"},{"id":14,"href":"/docs/device/hardware-build/","title":"Hardware Build","section":"ZIGGO Device","content":"\rHardware Build\r#\rBefore Start\r#\rIf you just want to teach the board to run instead of modifying it, and you use the hardware files we provide, you can skip directly to step SD card partition and copy file.\nTable of Content\r#\rHardware Build Before Start Table of Content Install Vivado Vivado project construction Petalinux 1.install ubuntu OS 2.Download PetaLinux related image files in advance 3.Activate the PetaLinux environment 4.Create PetaLinux Project 5.Petalinux Configuration 6. SD card partition and copy file File downloading SD card partition Copy files into SD card Launch the Board and log in 1. Launch the Board 2. Initialize PS 3. Connect to Internet 4. Run the Software Install Vivado\r#\rYou can download Vivado here.\nWe are using Vivado version 2020.1 (note: a unified version is required, otherwise running may cause problems).\nVivado project construction\r#\rclone the git repository for the hardware part. Open Vivado and enter the command cd Porject_Dir in Tcl Console and source ./tsn_device.tcl. After the command is executed, Vivado will automatically open the created project. Here, we will first close Vivado and rename the folder where the project is located to Work_ Dir, so that git can correctly ignore this directory and enter Work_ Dir, double-click the .xpr file to open the project again. Click Generate Bitstream Export the xsa file. In Vivado, File Export Export Hardware. Select Fixed for Platform type. Select include bitstream for Output. Click Finish to export the xsa file.\nExported xsa file path: Work_Dir/tns+tsn_device.xsa\nPetalinux\r#\r1.install ubuntu OS\r#\rThe following compilation processes are all completed within the virtual machine.\nWe recommend using the following configuration:\nVirtual machine version: VMware Workstation 16 Pro\rUbuntu version: ubuntu-16.04.3-desktop-amd64.iso 2.Download PetaLinux related image files in advance\r#\rEnter website: https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/embedded-design-tools/archive.html\nSelect 2020.1 to find PetaLinux Tools sstate-cache artifacts and download sstate_ arm_ 2020.1 and downloads and save the downloads in a specified directory in the virtual machine on January 2021 (the directory for this document is set to/home/alinx/data/)\n​ 3.Activate the PetaLinux environment\r#\rsource /opt/pkg/petalinux/settings.sh ​ Automatically activate the PetaLinux environment every time Terminal is started.\n4.Create PetaLinux Project\r#\rpetalinux-create --type project --template zynq --name \u0026lt;project_name\u0026gt; cd \u0026lt;project_name\u0026gt; 5.Petalinux Configuration\r#\rImport Hardware Configuration petalinux-config --get-hw-description \u0026lt;PATH-TO-XSA Directory\u0026gt; Then you will enter the menuconfig page and configure as follows:\nSubsystem AUTO Hardware Settings\r-\u0026gt; Ethernet Settings -\u0026gt; [*] Randomise MAC address\rImage Packaging Configurations -\u0026gt; Root filesystem type -\u0026gt; [*] EXT4 (SD/eMMC/SATA/USB)\rYocto Settings (Use downloaded image)\r-\u0026gt; Local sstate feeds settings: /home/alinx/data/sstate_arm_2020.1/arm\r-\u0026gt; Add pre-mirror url: file:///home/alinx/data/downloads make Kernel Module ​ make a module named \u0026ldquo;dma proxy\u0026rdquo;:\npetalinux-create -t modules -n dma-proxy --enable ​ replace the dma-proxy folder:\nrm -rf project-spec/meta-user/recipes-modules/* download our dma-proxy (from here) and unzip in project-spec/meta-user/recipes-modules/\nConfiguring the kernel petalinux-config -c kernel Entering the menuconfig page, and do:\nDevice Drivers -\u0026gt; Userspace I/O drivers -\u0026gt; [*] Userspace I/O platform driver with generic IRQ handling -\u0026gt; [*] Userspace platform driver with generic irq and dynamic memory -\u0026gt; [*] Xilinx AI Engine driverDevice Drivers -\u0026gt; Dma Engine Support -\u0026gt; [*] Xilinx DMA Engines -\u0026gt; Network device support -\u0026gt; Ethernet driver support -\u0026gt; [*] Cadence devices -\u0026gt; [*] Cadence MACB/GEM support General setup -\u0026gt; Preemption Model -\u0026gt; (X) No Forced Preemption (Server) Configuring rootfs Edit file project-spec/meta-user/conf/user-rootfsconfig, and add the following configuration:\nCONFIG_sudo CONFIG_sudo-dev CONFIG_dnf CONFIG_packagegroup-core-buildessential CONFIG_packagegroup-core-buildessential-dev CONFIG_packagegroup-self-hosted CONFIG_packagegroup-self-hosted-dev CONFIG_packagegroup-self-hosted-sdk-dev CONFIG_packagegroup-self-hosted-sdk CONFIG_python3 CONFIG_autoconf CONFIG_autoconf-dev CONFIG_automake CONFIG_automake-dev CONFIG_bison CONFIG_bison-dev CONFIG_flex CONFIG_flex-dev CONFIG_make CONFIG_make-dev CONFIG_python CONFIG_libtool CONFIG_libtool-dev CONFIG_sqlite3 CONFIG_cmake CONFIG_util-linux CONFIG_net-tools Save and exit.\nInput:\npetalinux-config -c rootfs Then you will enter the menuconfig page and configure as follows:\nImage Features -\u0026gt; [*] package management -\u0026gt; (http://petalinux.xilinx.com/sswreleases/rel-v2020/feeds/zc702-zynq7/) Package feed url.\ruser packages\r-\u0026gt; SELECT ALL Compile the device tree petalinux-build -c device-tree Modify the device tree The generated device tree is located in components/plnx_workspace/device tree/device tree/pl.dtsi directory, we need to make modifications based on this file to meet some of our peripheral needs. The modified files are stored in the project-spec/meta user/recipes bsp/device tree/files/system-user.dtsi directory.\nNote: Between the //\u0026gt;\u0026gt;\u0026gt; and //\u0026lt;\u0026lt;\u0026lt; is the content added between the comments.\n/include/ \u0026#34;system-conf.dtsi\u0026#34; / { amba_pl: amba_pl { dma_proxy { compatible =\u0026#34;xlnx,dma_proxy\u0026#34;; dmas = \u0026lt;\u0026amp;axi_dma_0 0 \u0026amp;axi_dma_0 1\u0026gt;; dma-names = \u0026#34;dma_proxy_tx\u0026#34;, \u0026#34;dma_proxy_rx\u0026#34;; }; pkt_gen_controller_0: pkt_gen_controller@43c00000 { compatible = \u0026#34;generic-uio\u0026#34;,\u0026#34;uio\u0026#34;; reg = \u0026lt;0x43c00000 0x10000\u0026gt;; }; rtc0: time_sync_uio@43c10000 { compatible = \u0026#34;generic-uio\u0026#34;,\u0026#34;uio\u0026#34;; reg = \u0026lt;0x43c10000 0x10000\u0026gt;; }; }; chosen{ bootargs = \u0026#34;console=ttyPS0,115200 earlyprintk cma=256M uio_pdrv_genirq.of_id=generic-uio root=/dev/mmcblk0p2 rw rootwait\u0026#34;; stdout-path = \u0026#34;serial0:115200n8\u0026#34;; }; }; \u0026amp;gem1 { local-mac-address = [00 00 00 00 02 01]; phy-mode = \u0026#34;gmii\u0026#34;; fixed-link { speed = \u0026lt;1000\u0026gt;; full-duplex; }; }; Complie petalinux-build Pack Here, you need to copy the bitstream file generated by Vivado to the virtual machine. (You can export bitstream file to where you want).\nThen:\ncd images/linux petalinux-package --boot --fsbl zynq_fsbl.elf --fpga \u0026lt;FPGA bitstream path\u0026gt; --u-boot --force After that, you can get 4 key file: BOOT.BIN boot.scr image.ub rootfs.tar.gz\n6. SD card partition and copy file\r#\rFile downloading\r#\rDownload the following file from this public link or from preceding steps:\nBOOT.BIN boot.scr image.ub rootfs.tar.gz SD card partition\r#\rIn order to boot the TSNPerf, you are supposed to have a micro SD card with \u0026gt;32GiB storage. Then use:\nsudo apt-get install gparted sudo gparted Parition it into two partition below\nBOOT: store boot files from petalinux\nFree space preceding (MiB): 4\nNew size (MiB): 500\nFile system: fat32\nLabel: BOOT\nROOTFS: store debian system rootfs\nFree space preceding (MiB): 0\nFree space following (MiB): 0\nFile system: ext4\nLabel: ROOTFS\nCopy files into SD card\r#\rMound SD card:\nsudo mount /dev/sda1 /media/alinx/BOOT/ sudo mount /dev/sda2 /media/alinx/ROOTFS/ Remove original files:\nsudo rm -rf /media/alinx/BOOT/* /media/alinx/ROOTFS/* Copy files:\nsudo cp BOOT.BIN boot.scr image.ub /media/alinx/BOOT sudo tar -zxvf rootfs.tar.gz -C /media/alinx/ROOTFS sudo cp -r ~/init_os.sh /media/alinx/ROOTFS/home/root/init_os.sh sync sudo chown root:root /media/alinx/ROOTFS sudo chmod 755 /media/alinx/ROOTFS Launch the Board and log in\r#\r1. Launch the Board\r#\rPlug the SD card into FPGA board, turn the switch to SD card boot mode.\n2. Initialize PS\r#\rPlug in SD card, setup AX7021 board to boot on SD, power on.\nConnect a PC to the UART port of the board. We recommend using MobaXterm to connect the serial. Set up the Speed to 115200, Flow Control to None.\nThe default username and password are as follows:\nusername: \u0026#34;root\u0026#34; password: \u0026#34;root\u0026#34; Execute the initilization script to set up the linux environment. (hardware/init_os.sh)\nsh init_os.sh You can freely configure the host name, IP address, and MAC address, etc with the script, and can modify the script if needed.\n3. Connect to Internet\r#\rConnect the PC\u0026rsquo;s network port to the device\u0026rsquo;s PS network port (ETH0).\nSet up PC\u0026rsquo;s corresponding port to be in the same subnet with the device (i.e., 192.168.137.x).\nAfterwards, you can connect to the device through ssh and copy any Software files needed.\n4. Run the Software\r#\rPlease refer to the software part of this repo for further instructions.\n"},{"id":15,"href":"/docs/device/software-build/","title":"Software Build","section":"ZIGGO Device","content":"\rSoftware Build\r#\rThis repo contains source code to enable TSN/CaaS TSNPerf\u0026rsquo; time synchronization logic and set up packet generation plan.\nTable of Content\r#\rSoftware Build Table of Content 1. Build 2. Config 3. Run 4. Analyze 4.1 TSNPerf directly analyzes latency and jitter 4.2 Offline analysis of latency and jitter 1. Build\r#\rmkdir build cd build cmake .. make After successfully build, there should be two executables: \u0026ldquo;time_sync\u0026rdquo; \u0026amp; \u0026ldquo;pkt_gen\u0026rdquo;\n2. Config\r#\rThe \u0026ldquo;build\u0026rdquo; directory contains topology \u0026amp; TSN/CaaS schedule results.\nconfig.json: topology file, contains node info (type, mac, ptp ports), link between nodes (with ports), and forwarding table. schedule.json: schedule file, contains each links\u0026rsquo; schedule time interval \u0026amp; each CaaS switch\u0026rsquo;s computation time interval. (Hints: This is similar to a Ziggo Switch)\n3. Run\r#\rStart time syncronization ./time_sync Start device critical packet generation ./pkt_gen 4. Analyze\r#\rThere are two analysis methods for TSNPerf:\nHint: If you want to use offline anaylze, please switch branch to \u0026lsquo;offline_anaylze\u0026rsquo;, and you need to link device to anthor PC(linux) by wire.\nOne method is to directly analyze latency and jitter on the TSNPerf.\nAnother method is to use Device to stamp the received packet and forward it from another port to a powerful desktop computer for packet capture analysis. Even if entering the Device at a gigabit rate, there will be no packet loss. The disadvantage is that a separate program needs to be written offline to analyze the delay and jitter of the packet.\n4.1 TSNPerf directly analyzes latency and jitter\r#\rDuring the operation of the time sync program, after receiving the test data frame, the program will save the delay information of the data frame in the package under the build/packet_log.csv file, save the batch statistics of latency and jitter in the critical directory under the build directory at the same time in the build/critical_log.csv.\nYou can display the content of the header and the last 5 lines using the following command:\ncat critical.log | head -n1 cat critical.log | tail -n5 4.2 Offline analysis of latency and jitter\r#\rIf conducting offline analysis, it is important to note that the hardware used is located at offline_analyze branch. This version will stamp the key data with a receive timestamp and forward it from ETH2. We need to use a PC equipped with a Linux system to capture packets for analysis.\nExample of packet capture command:\nsudo tcpdump -i enx207bd272812b ether src 00:0a:35:00:00:14 -n -B 100000 -w packets.pcapng Enx207bd272812b is the name of the network card, which can be obtained through ifconfig. Src is used to specify that the captured packet comes from a certain MAC address. -B is used to specify the buffer size. If packet loss needs to be set to a larger size, you can check the information output after packet capture to see if Dropped by kernel: is 0. If it is not 0, it indicates that there is packet loss in the kernel. -W specifies the path to save the file.\nCode for analyzing programs is analyze_packet.py\npython .\\analyze_packet.py [capture file path] --step [interval] At the same time, a packet will be generated in the current directory (packet_log.csv and critical_log.csv).\n"},{"id":16,"href":"/docs/device/ziggo_device_manual/","title":"ZIGGO Device使用手册","section":"ZIGGO Device","content":"\rZIGGO Device使用手册\r#\rThere are two analysis methods for ZIGGO Device:\nHint: If you want to use offline anaylze, please switch branch to \u0026lsquo;offline_anaylze\u0026rsquo;, and you need to link device to anthor PC(linux) by wire.\nOne method is to directly analyze latency and jitter on the ZIGGO Device. The advantage of this method is that it is convenient and fast. We can directly count the latency and jitter of all received data frames on the Device. However, due to the limited processing power of the development board CPU, when analyzing a large number of data packets (such as sending more than 10 1500B data packets at a time with a cycle of 33ms), packet loss may occur;\nAnother method is to use Device to stamp the received packet and forward it from another port to a powerful desktop computer for packet capture analysis. Even if entering the Device at a gigabit rate, there will be no packet loss. The disadvantage is that a separate program needs to be written offline to analyze the delay and jitter of the packet.\nZIGGO Device directly analyzes latency and jitter\r#\rDuring the operation of the time sync program, after receiving the test data frame, the program will save the delay information of the data frame in the package under the build/packet_log.csv file, save the batch statistics of latency and jitter in the critical directory under the build directory at the same time in the build/critical_log.csv.\nExample of packet_log.csv:\nSeq ID, Pkt ID, TX timestamp, RX timestamp, Latency\r0101, 0, 12683587446, 12683613722, 26276\r0101, 1, 12683599662, 12683625970, 26308\r0101, 2, 12683611862, 12683638162, 26300\r0101, 3, 12683624078, 12683650386, 26308\r0101, 4, 12717142039, 12717168273, 26234\r0101, 5, 12717154223, 12717180465, 26242\r0101, 6, 12717166391, 12717192625, 26234\r0101, 7, 12717178575, 12717204825, 26250\r0101, 8, 12750696471, 12750722712, 26241\r0101, 9, 12750708655, 12750734904, 26249 Example of critical_ log.csv, where Latency Variance is the variance of Latency:\nSeq ID, Received Number, Max pkt_id, Min pkt_id, Loss Rate, Latency Mean, Latency Variance\r0101, 1000, 0, 999, 0.000000, 26230, 1199\r0101, 1000, 1000, 1999, 0.000000, 26218, 2811\r0101, 1000, 2000, 2999, 0.000000, 26218, 2808\r0101, 1000, 3000, 3999, 0.000000, 26195, 18331\r0101, 1000, 4000, 4999, 0.000000, 26233, 921\r0101, 1000, 5000, 5999, 0.000000, 26226, 852\r0101, 1000, 6000, 6999, 0.000000, 26226, 5080\r0101, 1000, 7000, 7999, 0.000000, 26250, 105272\r0101, 1000, 8000, 8999, 0.000000, 26204, 7233\r0101, 1000, 9000, 9999, 0.000000, 26205, 7361 You can display the content of the header and the last 5 lines using the following command:\ncat critical.log | head -n1 cat critical.log | tail -n5 Offline analysis of latency and jitter\r#\rIf conducting offline analysis, it is important to note that the hardware used is located at offline_analyze branch. This version will stamp the key data with a receive timestamp and forward it from ETH2. We need to use a PC equipped with a Linux system to capture packets for analysis.\nExample of packet capture command:\nsudo tcpdump -i enx207bd272812b ether src 00:0a:35:00:00:14 -n -B 100000 -w packets.pcapng Enx207bd272812b is the name of the network card, which can be obtained through ifconfig. Src is used to specify that the captured packet comes from a certain MAC address. -B is used to specify the buffer size. If packet loss needs to be set to a larger size, you can check the information output after packet capture to see if Dropped by kernel: is 0. If it is not 0, it indicates that there is packet loss in the kernel. -W specifies the path to save the file.\nCode for analyzing programs is analyze_packet.py\npython .\\analyze_packet.py [capture file path] --step [interval] At the same time, a packet will be generated in the current directory (packet_log.csv and critical_log.csv).\n"},{"id":17,"href":"/docs/device/cnc_manual/","title":"CNC脚本使用指南","section":"ZIGGO Device","content":"\rCNC脚本使用指南\r#\r1. 运行脚本前，在batch.mjs中要配置好以下变量：\r#\r// IP address format: 192.168.137.* // 测试数据 const conf_default = \u0026#34;\u0026lt;需要执行的配置名称\u0026gt;\u0026#34; // 时钟主节点 const master = 40 const hosts = { device: [43, 44], switch: [40, 41], } 安装好需要的依赖：\r```bash\rsudo npm install -g zx@4.3.0\rpip install -r requirements.txt\rnpm install 配置好环境变量，在.bashrc中加入：\nexport PYTHONPATH=\u0026#34;/home/\u0026lt;用户名\u0026gt;/scripts\u0026#34; 2. 下载好Switch和TSNPerf的软件代码\r#\r（记得提前配置好树莓派和所有板子之间的ssh public key，方便免密登录）\n./batch.mjs clone 3. 拉取最新的软件代码并同步到远程板子上\r#\r默认时master分支，如果需要指定分支，需要修改batch.mjs文件中的pull()函数，具体参考注释的部分\n// pull all the latest code const pull = async () =\u0026gt; { cd(`${os.homedir()}/repos`) // 如果需要指定分支，类似下面指定qbv-test分支的操作 // await $`cd pkt_gen_app; git stash; git checkout qbv-test; git pull origin qbv-test; cd ..` // await $`cd time_sync_app; git stash; git checkout qbv-test; git pull origin qbv-test; cd ..` await Promise.all(hosts.all.map(host =\u0026gt; { const type = hosts.device.includes(host) ? \u0026#39;device\u0026#39; : \u0026#39;switch\u0026#39; if (type === \u0026#39;device\u0026#39;) { return $`rsync -avPh ${os.homedir}/repos/pkt_gen_app root@192.168.137.${host}:~/` } else if (type === \u0026#39;switch\u0026#39;) { return $`rsync -avPh ${os.homedir}/repos/time_sync_app root@192.168.137.${host}:~/;` } })) } 同步到所有板子上：\n./batch.mjs pull 4. 使远程所有板子编译代码\r#\r./batch.mjs build 5. 将配置文件下发到所有板子上\r#\r./batch.mjs distribute 6. 开始启动所有板子的程序\r#\r./batch.mjs launch 7. 收集延迟和抖动的统计信息\r#\r./batch.mjs collect "},{"id":18,"href":"/docs/device/testbed/","title":"TestBed搭建流程","section":"ZIGGO Device","content":"\rTestBed搭建流程\r#\r目录\r#\rTestBed搭建流程 目录 TestBed简单介绍 互联互通测试项目介绍 1. 基准测试 2. 门控能力 3. 带宽保证 4. 门控精度 设备参数设置与部署 TestBed运行流程 准备工作 控制脚本准备 背景流量发送 基准测试 门控能力 带宽保证 门控精度 TestBed简单介绍\r#\r我们内部对自己研发的CaaS-Switch进行了互联互通测试，采用的网络拓扑如下所示。\n本Demo需要4个FPGA开发板（2个TSNPerf、2个CaaS-Switch）和1台普通PC。如果条件有限/考虑简化拓扑，可以删去一个CaaS-Switch。PC负责发送背景流量，路径为黑色箭头；左侧TSNPerf负责发送测试流量（关键流量），路径为红色箭头。\n所有测试项目的流程大致可以分为两个阶段：\n时间同步：启动时间同步，保证TSNPerf、CaaS-Switch在一个时钟域下。\n注：时间同步程序运行在TSNPerf、CaaS-Switch的系统中。\n发送测试流量：根据schedule.json文件，TSNPerf进行测试流量发送。\n互联互通测试项目介绍\r#\r1. 基准测试\r#\r目的：测试无背景流量情况下，高优先级流量的两跳端到端延迟、丢包率、抖动，作为比较基准。 预期：延迟符合规划，抖动较低（100ns左右）。 2. 门控能力\r#\r目的：测试通用情况下流量调度效果，和两个交换机的门控对接能力。 方案： 背景流量（VLAN priority = 0）打到线速，每个调度周期预留长度为 3 的时隙给高优先级流量， 每个周期定时发送 4 个数据包长度为 1500Byte 的高优先级（VLAN priority = 1）测试流量，记录端到端平均延迟、抖动、丢包率、到达顺序 每个周期定时发送 8 个数据包长度为 1500Byte 的高优先级（VLAN priority = 1）测试流量，记录是否每个周期通过且仅 n 个包 预期： 延迟、抖动、丢包率和基准一致 每个周期定时发送 4 个数据包时，丢包率为0 每个周期定时发送 8 个数据包时，每个周期通过且仅通过 5 个包（每8个包丢3个，因为每个时隙里恰好可以把第五个包发出去，余下三个包丢掉），丢包率为37.5% 3. 带宽保证\r#\r目的：测试交换机能够进行带宽预留，且两个交换机优先级映射、运行 cycle 和 offset 基本一致。 方案： 背景流量（VLAN priority = 0）打到线速（1Gbps），配置交换机预留 50% 时隙给高优先级流量（VLAN priority = 1），其余 50% 只允许低优先级流量， 测试流量高优先级（VLAN priority = 1）情况下打到线速（经测试，每个周期[33ms]连续发送2752个1500B的包，基本达到1000Mbps），记录端到端平均延迟、抖动、丢包率、到达顺序 测试流量高优先级（VLAN priority = 1）情况下打到50%线速（每个周期[33ms]连续发送1376个1500B的包），记录端到端平均延迟、抖动、丢包率、到达顺序 预期： 高优先级测试流量100%线速：延迟、抖动和基准一致，丢包率 50% 左右 高优先级测试流量50%线速：延迟、抖动和基准一致，0丢包率 4. 门控精度\r#\r目的：测试门控时隙长度的设置精度 方案：(n=1,2,4,8,\u0026hellip;) 每个调度周期预留长度为 1 的时隙给高优先级流量 每个周期发送 n 个数据包长度为1/n*1500Byte的高优先级（VLAN priority = 1）测试流量，记录是否每个周期通过 n 个包 n=1, 预期： n=1,2,4,8 情况下都能每个周期通过且仅通过 n 个包 如果您按照下文的配置使用我们开源的ZIGGO交换机和TSNPerf，则您可以在上面四个测试中得到和预期相类似的效果。如果出现了任何问题，欢迎在github上提出issue。\n设备参数设置与部署\r#\rSwitch软硬件代码在仓库 ZIGGO-Caas-Switch 中，使用主分支即可。\nTSNPerf的软硬件代码在本仓库。\n注意：带宽能力测试中的TSNPerf需要使用offline_analyze分支，门控精度测试中的TSNPerf需要使用packet_resize分支【用于自定义测试包大小】。测试中请勿忘记切换分支。\n所有设备的IP、MAC和ID如下所示，与本文档后面提供的JSON配置文件对应：\n对应的config文件可见config.json\nTestBed运行流程\r#\r准备工作\r#\r控制脚本准备\r#\r可以参考 CNC-User-Manual\n背景流量发送\r#\r可以直接在Linux PC中运行send.py脚本。（需要安装scapy,tcpreplay）\n基准测试\r#\rSwitch、TSNPerf 的软硬件都使用主分支。\n测试步骤：\n修改batch.mjs修改正确的配置文件名 ./batch.mjs pull（注意分支正确） ./batch.mjs distribute Device每次发送4个数据包，每个Switch都预留了3个时隙，将以下两个配置文件下发即可： base/d3s2-baseline-3t-config.json， base/d3s2-baseline-3t-schedule-base.json ./batch.mjs launch （包括了时间同步和数据包发送） ./batch.mjs collect 门控能力\r#\rSwitch、TSNPerf 的软硬件都使用主分支。\n让背景流量Device发包（基准测试不需要发送背景流量） 修改batch.mjs修改正确的配置文件名 ./batch.mjs pull（注意分支正确） ./batch.mjs distribute Device1每次发送4个数据包，每个Switch都预留了3个时隙，将以下两个配置文件下发即可： gate/d3s2-baseline-3t-config.json， gate/d3s2-baseline-3t-schedule-gate.json ./batch.mjs launch ./batch.mjs collect 将上述的schedule.json中的Device发包个数从4改为8，再从上述步骤4重新测试，修改后的schedule.json如下所示 { \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, \u0026#34;from\u0026#34;: 3, \u0026#34;to\u0026#34;: 0, \u0026#34;from_port\u0026#34;: 0, \u0026#34;id\u0026#34;: 7, \u0026#34;schedule\u0026#34;: [ { \u0026#34;period\u0026#34;: 2048, \u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 8, \u0026#34;job_id\u0026#34;: 1, \u0026#34;flow_id\u0026#34;: 1 } ] }, 带宽保证\r#\rSwitch代码使用主分支，TSNPerf代码使用offline_analyze分支。\n测试步骤：\n让背景流量Device发包（基准测试不需要发送背景流量）\n修改batch.mjs修改正确的配置文件名\n./batch.mjs pull（注意分支正确）\n./batch.mjs distribute Device1每次发送1376个数据包，每个Switch都预留了1024个时隙，将以下两个配置文件下发即可： bandwidth/d3s2-baseline-50%t-config.json， bandwidth/d3s2-baseline-50%t-schedule.json\n./batch.mjs launch\n使用离线分析\n将上述的schedule.json中的Device发包个数从1376改为2752，再从上述步骤4重新测试，修改后的schedule.json如下所示\n{ \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, \u0026#34;from\u0026#34;: 3, \u0026#34;to\u0026#34;: 0, \u0026#34;from_port\u0026#34;: 0, \u0026#34;id\u0026#34;: 7, \u0026#34;schedule\u0026#34;: [ { \u0026#34;period\u0026#34;: 2048, \u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 2752, \u0026#34;job_id\u0026#34;: 1, \u0026#34;flow_id\u0026#34;: 1 } ] }, 门控精度\r#\rSwitch代码使用主分支，TSNPerf代码使用packet_resize分支。\n修改batch.mjs修改正确的配置文件名\n./batch.mjs pull（注意分支正确）\n./batch.mjs distribute Device1每次发送1个1500B的数据包，每个Switch都预留了1个时隙，将以下两个配置文件下发即可： accuracy/d3s2-baseline-1t-config.json， accuracy/d3s2-baseline-1t-schedule.json\n./batch.mjs launch\n./batch.mjs collect\n将上述的schedule.json中的Device发包个数从1分别改为2,4,8，包长度分别改为750, 375和187，再从上述步骤4重新测试，发包个数修改为2后的schedule.json如下所示\n{ \u0026#34;type\u0026#34;: \u0026#34;link\u0026#34;, \u0026#34;from\u0026#34;: 3, \u0026#34;to\u0026#34;: 0, \u0026#34;from_port\u0026#34;: 0, \u0026#34;id\u0026#34;: 7, \u0026#34;schedule\u0026#34;: [ { \u0026#34;period\u0026#34;: 2048, \u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 2, \u0026#34;pkt_size\u0026#34;: 750, // 1500/2 = 750 \u0026#34;job_id\u0026#34;: 1, \u0026#34;flow_id\u0026#34;: 1 } ] }, "},{"id":19,"href":"/docs/tsnperf/manual/","title":"TSNPerf 检测内容及使用表","section":"ZIGGO TSNPerf","content":"\rTSNPerf 检测内容及使用表\r#\r流量整形测试\r#\r本节将介绍使用TSNPerf测试待测设备流量整形功能的方法。本章节中的测试参数均为参考配置，使用者可根据实际需求修改参数。\n基准测试\r#\r目的： 测试 无 背景流量情况下，高优先级流量经过一跳交换机（待测设备）的端到端延迟、抖动和丢包率。\n方案： 使用如下拓扑，按照配置文档中的介绍发送关键数据包，记录关键数据包的端到端时延、抖动和丢包率。\n下面给出TSNPerf程序的参考参数配置：\nmode: 0 tx-mode: 2 verbose: true use-ziggo-analysis: false pcap-filename: \u0026#34;/home/i210/launchtimedemo/captured_10w_1500Byte.pcap\u0026#34; interface: \u0026#34;enp1s0\u0026#34; smac: \u0026#34;00:1b:21:77:ac:ae\u0026#34; dmac: \u0026#34;00:1b:21:76:ae:75\u0026#34; ethertype: 0xb62c socket-priority: 0 vlan-priority: 0 offset: 150000 early-offset: 300000 use-launchtime: true basetime: 1684559640000000100L packet-size: 1500 packets-to-send: 10000 interval: 1000000 use-udp: false sip: \u0026#34;192.168.16.10\u0026#34; dip: \u0026#34;192.168.16.11\u0026#34; sport: 10000 dport: 10000 该配置中，packet-size可以根据需要改变，推荐可以设置为 64 Byte（最小以太网帧长）、100 Byte、500 Byte、1000 Byte、1500 Byte等。\n每一个关键数据包中，从第26个字节开始的8个字节是数据包的发送时间戳，用tcpdump抓取对应的接收时间戳，进而求得每个数据包的端到端时延。统计发送出的10000个数据包，可以得到平均时延和抖动（方差/标准差），以及丢包率。\n门控能力\r#\r目的： 测试有背景流量情况下，高优先级流量经过一跳交换机（待测设备）的端到端延迟、抖动和丢包率。\n方案： 使用如下拓扑进行测试。从Injector向Recorder发送背景流量，同时从Publisher向Recorder发送关键流量，记录关键数据包的端到端时延、抖动和丢包率。\n为了全面测试待测设备地门控能力，可以调整Injector向网络中注入的背景流量带宽，以千兆网络为例，可以分别检查背景流量为 250 Mbps、500 Mbps、750 Mbps、1000 Mbps的情况下，关键数据包的时延抖动。\nTSNPerf程序的配置参数可参考上一小节。如果待测设备功能正常，则测试结果应与基准测试中得到的结果一致。\n门控精度\r#\r目的： 测试交换机门控的精度，即能否通过预期数量和大小的以太网帧\n方案： 每个周期预留16384ns的门控给关键流量，理论上每个周期可以通过2个1500Byte的以太网帧。\n下面给出TSNPerf程序的参考参数配置：\nmode: 0 tx-mode: 3 verbose: true use-ziggo-analysis: false pcap-filename: \u0026#34;/home/i210/launchtimedemo/captured_10w_1500Byte.pcap\u0026#34; interface: \u0026#34;enp1s0\u0026#34; smac: \u0026#34;00:1b:21:77:ac:ae\u0026#34; dmac: \u0026#34;00:1b:21:76:ae:75\u0026#34; ethertype: 0xb62c socket-priority: 0 vlan-priority: 0 offset: 150000 early-offset: 300000 use-launchtime: false basetime: 1684559640000000100L packet-size: 1500 packets-to-send: 100000 interval: 1000000 use-udp: false sip: \u0026#34;192.168.16.10\u0026#34; dip: \u0026#34;192.168.16.11\u0026#34; sport: 10000 dport: 10000 Publisher向Recorder打满关键流量，Recorder处统计接收到的关键数据包的时间戳。预期每个周期（1毫秒）可以接收到两个关键数据包。\n另外，还可以在不同的拓扑下，更细粒度地验证待测设备的门控精度。在下图拓扑上，以千兆以太网、1500Byte测试包为例，我们配置 DUT1 在周期的 0~16us 打开关键流量门控，DUT2 在周期的 15~31us 打开关键流量门控吗。期望，关键数据包能够不丢包地通过网络设备，并且端到端时延与门控全开时的结果一致。\n带宽保障\r#\r目的： 测试待测交换机是否可以为关键流量预留固定的带宽\n方案：\n配置待测交换机的门控，为关键数据预留25%、50%、75%的带宽。 从Injector处向Recorder打满背景流量。 用TSNPerf，从Publisher向Recorder发送关键流量，配置参数可参考上一小节。 Recorder处用tcpdump/wireshark记录带宽使用情况。以千兆带宽为例，预期关键数据会分别占用 250Mbps、500Mbps、750Mbps 的带宽。 视频流量测试\r#\r目的： 测试以太网帧大小不同的视频流量经过一跳交换机的端延迟、抖动和丢包率\n方案： 测试拓扑参考基准测试章节。准备好一个视频流量的pcap文件H264.pcap，TSNPerf程序的参数配置如下：\nmode: 0 tx-mode: 0 verbose: true use-ziggo-analysis: false pcap-filename: \u0026#34;/home/i210/launchtimedemo/H264.pcap\u0026#34; interface: \u0026#34;enp1s0\u0026#34; smac: \u0026#34;00:1b:21:77:ac:ae\u0026#34; dmac: \u0026#34;00:1b:21:76:ae:75\u0026#34; ethertype: 0xb62c socket-priority: 0 vlan-priority: 0 offset: 150000 early-offset: 300000 use-launchtime: true basetime: 1684559640000000100L packet-size: 1500 packets-to-send: 10000 interval: 1000000 use-udp: true sip: \u0026#34;192.168.16.10\u0026#34; dip: \u0026#34;192.168.16.11\u0026#34; sport: 10000 dport: 10000 如果待测交换机是存储转发（store-and-forward）模式的，则预期每个数据包的端到端时延会因包的大小而有所不同。如果待测交换机是直通转发（cut-through）模式的，则预期数据包的端到端时延抖动是很小的。\n超参数汇总\r#\r除了上文中给的TSNPerf程序配置，还有一些参数在测试用例中经常需要根据需求改动，现将之罗列于下：\npacket-size：该参数设置了测试包的大小，用于验证待测设备转发不同数据包的能力。推荐的配置有：64 Byte（最小以太网帧长）、100 Byte、500 Byte、1000 Byte、1500 Byte等。 背景流量大小：该参数设置了网络中的背景流量大小，用于验证待测设备能否保障关键流量的低延迟、低抖动传输。可以根据实际网络带宽的大小，配置背景流量分别占用 0%、25%、50%、75%、100% 的网络带宽。 拓扑：拓扑决定了关键流量在网络中的传输路径。在不同拓扑上测试可以更加全面地验证待测设备的门控能力、门控精度。本文限于篇幅，仅使用了 一跳测试拓扑 和 两跳测试拓扑，使用者可以根据需要，改变测试拓扑。 "},{"id":20,"href":"/docs/device/contributing/","title":"How to contribute","section":"ZIGGO Device","content":"\rHow to contribute\r#\rReporting a bug\r#\rFile bugs in the Github Issue Tracker. Please follow these guidelines:\nSearch existing issues first, make sure yours hasn\u0026rsquo;t already been reported. Consider enabling debug mode so that you can provide as much details as possible when reporting the issue. Stay on topic, but describe the issue in detail so that others can reproduce it. Provide a screenshot if possible. A screenshot showing the problem is often more useful than a paragraph describing it. The development team will only answer questions on github issues and reject other forms of questions.\nContributing to ZIGGO\u0026rsquo;s code\r#\rIf you want to start contributing to the project\u0026rsquo;s code, please follow these guidelines before creating a pull request:\nThe top post of the pull request should contain a full, self-contained explanation of the feature: what it does, how it does it, with examples of usage and screenshots. Also explain why you want to add this - what problem does it solve. Do not simply add a text Implement feature #4345 , because the information there will most likely be outdated or confusing (multiple discussions and opinions). The pull request needs to be self-contained. Bug fixes are always welcome. A good way to easily start contributing is to pick and work on a good first issue. We try to make these issues as clear as possible and provide basic info on how the code should be changed, and if something is unclear feel free to ask for more information on the issue. Before adding a new feature, ask about it in the Github Issue Tracker , or check if existing discussions exist to make sure the new functionality is desired. Pull requests that make many changes using an automated tool, like for spell fixing, styling, etc. will not be accepted. Pull requests that address multiple issues will most likely stall and eventually be closed. This is because we might be fine with one of the changes but not with others and untangling that kind of pull request is too much hassle both for maintainers and the person who submitted it. So most of the time someone gives up and the PR gets closed. So please keep the pull request focused on one issue. Do not mark your reviewer\u0026rsquo;s comments as \u0026ldquo;resolved\u0026rdquo;. If you do that, the comments will be hidden and the reviewer will not know what are the pending issues in the pull request. Only the reviewer should resolve the comments. "},{"id":21,"href":"/docs/tsnperf/report/","title":"某品牌交换机报告","section":"ZIGGO TSNPerf","content":"\r某品牌交换机报告\r#\r详细报告请查看这里。\n"},{"id":22,"href":"/docs/device/","title":"ZIGGO Device","section":"Docs","content":" ZIGGO Device: A flexible and standard-compliant toolkit for TSN performance evaluation.\r#\rTable of Contents\r#\rZIGGO Device: A flexible and standard-compliant toolkit for TSN performance evaluation. Table of Contents Introduction ZIGGO Open Platform Demo Features Read before start Getting Started System Design Demo APP Tutorial License and Citation TODO List Contributing Introduction\r#\rZIGGO is a flexible, standard-compliant, and control-function-virtualized TSN switch platform ready for industrial control, automotive electronics, and other time-sensitive applications.\nThis is the document for the ZIGGO Device. (We also offer ZIGGO-CaaS-Switch that comply with the IEEE 802.1 TSN standard.) Our Device supports testing all standards-compliant switches.\nZIGGO Open Platform\r#\rThe construction of the ZIGGO Open Platform consists of three levels: network device, management tools, and a Demo App:\nThe software and hardware projects, along with the development board startup tutorial, provide instructions for setting up an individual network device.\nThe CNC User Manual and Device User Manual cover system configuration and management tools.\nLastly, we offer a comprehensive Demo App building tutorial that instructs how to collaboratively build a complete and functional Demo using network devices and management tools.\nDemo\r#\rWe provide a demonstration video of the TSN switch. It demonstrates the superior performance of the ZIGGO-CaaS-Switch compared to the normal switch.\nThe left side of the picture is the ZYNQ development board we use, and the right side is the TSN display board we built.\nClick the pic to watch the video! Or just click here.\nFeatures\r#\rZIGGO supports the simultaneous transmission of both Information Technology (IT) and Operation Technology (OT) data traffic with QoS guarantee.\nZIGGO complies with IEEE standards 802.1AS, Qav, Qbv, and Qcc.\nZIGGO provides Real-time and Deterministic Ethernet transport\nZIGGO achieve Zero Packet Loss , Microsecond-level Latency with Nanosecond-level Jitter Gate Ability. ZIGGO guarantee Gigabit Throughput. ZIGGO provide gate accuracy applicable to All Ethernet Frame Sizes. Read before start\r#\rGetting started with ZIGGO-CaaS-Switch/ZIGGO-Device is a pretty hard task. Users/developers need to have sufficient basic knowledge and be prepare to for a long periond of learning and debugging.\nPlease refer to basic_knowledge.md to check if you have ability to use ZIGGO competently.\nGetting Started\r#\rPlease refer to required.md to get prepared.\nAfter that, please refer to getting_started.md for the build and run a single ZIGGO Device.\nSystem Design\r#\rZIGGO is implemented on ZYNQ-7000 SoC and exploits ZYNQ\u0026rsquo;s both hardware and software programmability.\nWe also provide more in-depth documentation explaining specific design principles for ZIGGO Device.\nDemo APP Tutorial\r#\rWe also provide a testbed build document that allows you to build a real-time Ethernet system using the ZIGGO swtich and Device.\nThrough this platform, we can measure the delay and jitter of TSN time-critcial traffic, the switch\u0026rsquo;s gating capability, bandwidth guarantee and gating accuracy.\nReplacing ZIGGO CaaS switches with commercial TSN switches can also test its above capabilities.\nLicense and Citation\r#\rZIGGO is released under a MIT license.\nPlease consider citing our papers if the project helps your research with the following BibTex:\n@inproceedings{caas, author={Yang, Zheng and Zhao, Yi and Dang, Fan and He, Xiaowu and Wu, Jiahang and Cao, Hao and Wang, Zeyu and Liu, Yunhao}, booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}, title={CaaS: Enabling Control-as-a-Service for Time-Sensitive Networking}, year={2023}, pages={1-10}, doi={10.1109/INFOCOM53939.2023.10228980}} @inproceedings{etsn, author={Zhao, Yi and Yang, Zheng and He, Xiaowu and Wu, Jiahang and Cao, Hao and Dong, Liang and Dang, Fan and Liu, Yunhao}, booktitle={IEEE ICDCS 2022 - IEEE International Conference on Distributed Computing Systems}, title={E-TSN: Enabling Event-triggered Critical Traffic in Time-Sensitive Networking for Industrial Applications}, year={2022}, volume={}, number={}, pages={691-701}, doi={10.1109/ICDCS54860.2022.00072}} TODO List\r#\rZIGGO CaaS Switch Release ZIGGO Device Release ZIGGO Device Source Code Tutorial for build a testbed Test Case for TSN We will expand each test in the tutorial to multiple test cases to cover different edge cases and comprehensively test the performance of TSN switches.\nSupport Device List At present, we have only tested our own Ziggo switches and are testing other commercial switches (such as Huawei ,H3C and NXP). We expect to maintain a list of test results in the future.\nContributing\r#\rPlease see the guide for information on how to ask for help or contribute to the development of ZIGGO!\nThe development team will only answer questions on github issues and reject other forms of questions.\n"},{"id":23,"href":"/menu/","title":"Index","section":"ZIGGO BOOK","content":" "}]